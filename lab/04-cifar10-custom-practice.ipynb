{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-21T11:53:33.343874Z","iopub.execute_input":"2021-07-21T11:53:33.344277Z","iopub.status.idle":"2021-07-21T11:53:33.355435Z","shell.execute_reply.started":"2021-07-21T11:53:33.344189Z","shell.execute_reply":"2021-07-21T11:53:33.354283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CIFAR10 Dataset 생성 \n* tf.keras.datasets의 cifar10.load_data()는 웹에서 Local computer로 Download후 train과 test용 image와 label array로 로딩. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\n# 전체 6만개 데이터 중, 5만개는 학습 데이터용, 1만개는 테스트 데이터용으로 분리\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n# (50000, 32, 32, 3): 이미지 개수 50000개, 이미지 크기 32*32, 이미지 채널 3개(RGB) \n# (50000, 1): 2차원 -> 정확하게 하려면 1차원으로 바꿔주는게 좋음 \nprint(\"train dataset shape:\", train_images.shape, train_labels.shape)\nprint(\"test dataset shape:\", test_images.shape, test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:53:33.360996Z","iopub.execute_input":"2021-07-21T11:53:33.361457Z","iopub.status.idle":"2021-07-21T11:53:43.56806Z","shell.execute_reply.started":"2021-07-21T11:53:33.361413Z","shell.execute_reply":"2021-07-21T11:53:43.566968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images[0, :, :, :], train_labels[0, :]","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:53:43.569783Z","iopub.execute_input":"2021-07-21T11:53:43.570392Z","iopub.status.idle":"2021-07-21T11:53:43.585845Z","shell.execute_reply.started":"2021-07-21T11:53:43.570347Z","shell.execute_reply":"2021-07-21T11:53:43.58488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NAMES = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\nprint(train_labels[:10])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:53:43.592887Z","iopub.execute_input":"2021-07-21T11:53:43.593278Z","iopub.status.idle":"2021-07-21T11:53:43.835562Z","shell.execute_reply.started":"2021-07-21T11:53:43.593238Z","shell.execute_reply":"2021-07-21T11:53:43.834419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CIFAR10 데이터 시각화\n* 이미지 크기는 32x32이며 RGB채널. \n* 전반적으로 Label에 해당하는 대상이 이미지의 중앙에 있고, Label 대상 오브젝트 위주로 이미지가 구성. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n%matplotlib inline \n\ndef show_images(images, labels, ncols=8):\n    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        axs[i].imshow(images[i])\n        label = labels[i].squeeze() # squeeze: 2차원 -> 1차원\n        axs[i].set_title(NAMES[int(label)])\n        \nshow_images(train_images[:8], train_labels[:8], ncols=8)\nshow_images(train_images[8:16], train_labels[8:16], ncols=8)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:00:30.124868Z","iopub.execute_input":"2021-07-21T12:00:30.125243Z","iopub.status.idle":"2021-07-21T12:00:32.173567Z","shell.execute_reply.started":"2021-07-21T12:00:30.125209Z","shell.execute_reply":"2021-07-21T12:00:32.172654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data preprocessing\n* image array의 0 ~ 255 사이의 값으로 되어 있는 pixel intensity 값을 0 ~ 1 사이 값으로 변환. 정수값 pixel 값을 255.0 으로 나눔. \n* label array는 숫자형 값으로 바꾸되, 원-핫 인코딩을 적용할지 선택. 일반적으로 원-핫 인코딩을 적용하는게 Keras Framework활용이 용이\n* image array, label array 모두 float32 형으로 변환. numpy 의 float32는 tensor 변환시 tf.float32 로 변환되며 기본적으로 Tensorflow backend Keras는 tf.float32를 기반으로 함. \n","metadata":{}},{"cell_type":"code","source":"(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# label은 원-핫 인코딩이 Keras에서는 활용이 용이하나, 여기서는 sparse categorical crossentropy 테스트를 위해 적용하지 않음. \n# categorical -> 원-핫 인코딩(권장)\n# sparse -> 원-핫 X\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\n# 반드시 test도 똑같이 해줘야 함!\ntrain_images, train_labels = get_preprocessed_data(train_images, train_labels)\ntest_images, test_labels = get_preprocessed_data(test_images, test_labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:02:39.644186Z","iopub.execute_input":"2021-07-21T12:02:39.644535Z","iopub.status.idle":"2021-07-21T12:02:41.080029Z","shell.execute_reply.started":"2021-07-21T12:02:39.644506Z","shell.execute_reply":"2021-07-21T12:02:41.079168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images[0, :, :, :]","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:02:54.89008Z","iopub.execute_input":"2021-07-21T12:02:54.890475Z","iopub.status.idle":"2021-07-21T12:02:54.898523Z","shell.execute_reply.started":"2021-07-21T12:02:54.890432Z","shell.execute_reply":"2021-07-21T12:02:54.897545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keras는 CNN(정확히는 CNN 2D) 모델에 학습 데이터를 입력할 시 반드시 Image array는 4차원 배열이 되어야 함. \n# RGB 채널 이미지 array는 기본적으로 3차원임. 여기에 이미지의 갯수를 포함하므로 4차원이 됨.  \n# 만일 Grayscale인 2차원 이미지 array라도 의도적으로 채널을 명시해서 3차원으로 만들어 주고, 여기에 이미지 개수를 포함해서 4차원이 됨. \n\nprint(train_images.shape, train_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:03:37.152352Z","iopub.execute_input":"2021-07-21T12:03:37.152716Z","iopub.status.idle":"2021-07-21T12:03:37.157993Z","shell.execute_reply.started":"2021-07-21T12:03:37.152685Z","shell.execute_reply":"2021-07-21T12:03:37.156948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label 데이터가 2차원임. 이를 Keras 모델에 입력해도 별 문제없이 동작하지만, label의 경우는 OHE적용이 안되었는지를 알 수 있게 명확하게 1차원으로 표현해 주는것이 좋음. \n# 2차원인 labels 데이터를 1차원으로 변경. \ntrain_labels = train_labels.squeeze()\ntest_labels = test_labels.squeeze()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:03:39.85382Z","iopub.execute_input":"2021-07-21T12:03:39.854162Z","iopub.status.idle":"2021-07-21T12:03:39.858644Z","shell.execute_reply.started":"2021-07-21T12:03:39.854112Z","shell.execute_reply":"2021-07-21T12:03:39.857623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom Model 생성\n* CNN Model의 맨처음 Layer는 Input layer. Input layer의 shape를 이미지 사이즈와 RGB 3채널에 맞게 (32, 32, 3) 으로 설정.\n* Conv 연산을 연달아 적용하고 MaxPooling을 적용하는 루틴으로 모델 생성. MaxPooling을 적용 후에는 필터 갯수를 더욱 증가 시킴. \n* MaxPooling 적용 후에 출력 피처맵의 사이즈는 작아지되, 채널(깊이)는 늘어나는 형태로 모델 생성. \n* CIFAR10의 Label수가 10개이므로 Classification을 위한 맨 마지막 Dense layer의 units 갯수는 10개임\n* label값이 원-핫 인코딩 되지 않았기 때문에 model.compile()에서 loss는 반드시 sparse_categorical_crossentropy여야함. \n* 만일 label값이 원-핫 인코딩 되었다면 loss는 categorical_crossentropy 임. ","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:04:13.172778Z","iopub.execute_input":"2021-07-21T12:04:13.173151Z","iopub.status.idle":"2021-07-21T12:04:13.177394Z","shell.execute_reply.started":"2021-07-21T12:04:13.173101Z","shell.execute_reply":"2021-07-21T12:04:13.176388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) # shape가 3차원으로 보인다고 3차원이 아니야! 이미지 개수(배치)까지 4차원이야 -> 케라스가 알아서 조정\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor) # 필터 사이즈 조정: 요즘엔 보통 3*3\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)\nx = Activation('relu')(x) # Conv2D의 인자로 안주고 이렇게 따로 할 수도 있음\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(x)\nx = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary() # model의 output = feature map","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:07:21.412682Z","iopub.execute_input":"2021-07-21T12:07:21.413074Z","iopub.status.idle":"2021-07-21T12:07:23.578582Z","shell.execute_reply.started":"2021-07-21T12:07:21.413043Z","shell.execute_reply":"2021-07-21T12:07:23.577681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer는 Adam으로 설정하고, label값이 원-핫 인코딩이 아니므로 loss는 sparse_categorical_crossentropy 임. \nmodel.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:09:48.041281Z","iopub.execute_input":"2021-07-21T12:09:48.041653Z","iopub.status.idle":"2021-07-21T12:09:48.060648Z","shell.execute_reply.started":"2021-07-21T12:09:48.041622Z","shell.execute_reply":"2021-07-21T12:09:48.059434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 학습 수행 및 테스트 데이터로 평가 \n* Model의 fit() 메소드를 호출하여 학습\n* fit()은 학습 데이터가 Numpy array 자체로 들어올때, Generator 형태로 들어올때 약간의 수행로직 차이가 있음. \n* 인자로 x에는 학습 image data, y는 학습 label 데이터. \n* batch_size는 한번에 가져올 image/label array 갯수. 1개씩 가져오면 수행속도가 너무 느리고, 전체를 가져오면 GPU Memory 부족이 발생할 수 있으므로 적절한 batch_size 설정이 필요. 만약 학습 데이터가 generator일 경우, fit()에서 batch_size를 설정하지 않음. \n* epochs 는 전체 학습 데이터 학습을 반복 수행할 횟수\n* steps_per_epoch는 전체 학습 데이터를 몇번 배치 작업으로 수행하는가를 의미. 보통 입력데이터가 generator일 경우 설정. \n* validation_data는 검증용 데이터 세트\n* validation_steps는 검증용 데이터의 steps_per_epoch임. \n* validation_split는 validation_data로 별도의 검증용 데이터 세트를 설정하지 않고 자동으로 학습용 데이터에서 검증용 데이터 세트 분할. \n","metadata":{}},{"cell_type":"code","source":"history = model.fit(x=train_images, y=train_labels, batch_size=64, epochs=30, validation_split=0.15)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:09:52.476388Z","iopub.execute_input":"2021-07-21T12:09:52.476718Z","iopub.status.idle":"2021-07-21T12:12:15.436741Z","shell.execute_reply.started":"2021-07-21T12:09:52.476686Z","shell.execute_reply":"2021-07-21T12:12:15.4356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history(history):\n    plt.figure(figsize=(6, 6))\n    plt.yticks(np.arange(0, 1, 0.05))\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend()\n    \nshow_history(history)\n\n# 테스트 데이터로 성능 평가\nmodel.evaluate(test_images, test_labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:12:19.361464Z","iopub.execute_input":"2021-07-21T12:12:19.361818Z","iopub.status.idle":"2021-07-21T12:12:20.769949Z","shell.execute_reply.started":"2021-07-21T12:12:19.361788Z","shell.execute_reply":"2021-07-21T12:12:20.769074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model.predict()를 통해 이미지 분류 예측\n* 4차원 이미지 배열을 입력해서 모델학습함. predict()시에도 4차원 이미지 배열을 입력해야함. \n* 학습 데이터의 원-핫 인코딩 적용 여부와 관계없이 softmax 적용 결과는 무조건 2차원 임에 유의  ","metadata":{}},{"cell_type":"code","source":"# 아래 코드는 오류 발생. Conv2D를 사용한 모델에 4차원 이미지 배열을 입력해서 모델을 학습했으므로 predict()시에도 테스트용 4차원 이미지 배열을 입력해야 함.  \n# 학습 차원과 예측 차원이 같아야 함! -> 하나의 이미지를 넣을 때도 4차원으로 넣어줘야 함\npreds = model.predict(test_images[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:12:23.090646Z","iopub.execute_input":"2021-07-21T12:12:23.090993Z","iopub.status.idle":"2021-07-21T12:12:23.214763Z","shell.execute_reply.started":"2021-07-21T12:12:23.090962Z","shell.execute_reply":"2021-07-21T12:12:23.211641Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 테스트용 4차원 이미지 배열을 입력해서 predict()수행. \n# predict()의 결과는 softmax 적용 결과임. 학습 데이터의 원-핫 인코딩 적용 여부와 관계없이 softmax 적용 결과는 무조건 2차원임에 유의 \n# 즉, sparse-categorical-loss 적용했다고 하나의 레이블만 골라주는게 아님 -> 각각의 label의 확률을 2차원으로 출력\npreds = model.predict(np.expand_dims(test_images[0], axis=0))\nprint('예측 결과 shape:', preds.shape)\nprint('예측 결과:', preds) # softmax: 2차원 ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:12:23.444467Z","iopub.execute_input":"2021-07-21T12:12:23.444806Z","iopub.status.idle":"2021-07-21T12:12:23.609358Z","shell.execute_reply.started":"2021-07-21T12:12:23.444775Z","shell.execute_reply":"2021-07-21T12:12:23.608104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test_images[:32], batch_size=32)\nprint('예측 결과 shape:', preds.shape)\nprint('예측 결과:', preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:13:36.017799Z","iopub.execute_input":"2021-07-21T12:13:36.018244Z","iopub.status.idle":"2021-07-21T12:13:36.069094Z","shell.execute_reply.started":"2021-07-21T12:13:36.018176Z","shell.execute_reply":"2021-07-21T12:13:36.068222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_class = np.argmax(preds, axis=1) # 값을 원한다면 argmax를 이용해 값을 뽑아줘야 함!\nprint('예측 클래스 값:', predicted_class)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:13:40.160044Z","iopub.execute_input":"2021-07-21T12:13:40.160424Z","iopub.status.idle":"2021-07-21T12:13:40.166224Z","shell.execute_reply.started":"2021-07-21T12:13:40.160393Z","shell.execute_reply":"2021-07-21T12:13:40.165254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(test_images[:8], predicted_class[:8], ncols=8)\nshow_images(test_images[:8], test_labels[:8], ncols=8)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:13:41.154106Z","iopub.execute_input":"2021-07-21T12:13:41.154526Z","iopub.status.idle":"2021-07-21T12:13:42.843455Z","shell.execute_reply.started":"2021-07-21T12:13:41.154495Z","shell.execute_reply":"2021-07-21T12:13:42.842396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 평균이 1 이고 표준편차가 1인 표준 정규분포에서 난수 추출\n* 표준 편차가 클 수록 개별 값의 크기가 일반적으로 커짐.","metadata":{}},{"cell_type":"code","source":"numbers = np.random.normal(loc=0.0,scale=1,size=[100, 100]) # 표준편차 ~= scale\nprint(numbers)\nprint(numbers.mean())\nprint(numbers.std())\nprint(numbers.sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:24:17.493948Z","iopub.execute_input":"2021-07-21T12:24:17.49435Z","iopub.status.idle":"2021-07-21T12:24:17.5046Z","shell.execute_reply.started":"2021-07-21T12:24:17.494314Z","shell.execute_reply":"2021-07-21T12:24:17.503232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Xavier initialization - 정규분포(glorot_normal), 균일분포(glorot_uniform) ","metadata":{}},{"cell_type":"code","source":"# glorot_normal\nfan_in = 20\nfan_out = 15\nscale_value = np.sqrt(2/(fan_in + fan_out))\nprint('scale:', scale_value)\nweights = np.random.normal(loc=0.0, scale=scale_value, size=(100, 100))\nprint(weights)\nprint('weights mean:',weights.mean(), 'std:', weights.std(), 'sum:', weights.sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:30:17.193639Z","iopub.execute_input":"2021-07-21T12:30:17.193983Z","iopub.status.idle":"2021-07-21T12:30:17.209445Z","shell.execute_reply.started":"2021-07-21T12:30:17.19395Z","shell.execute_reply":"2021-07-21T12:30:17.206747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# glorot_uniform\nfan_in = 10\nfan_out = 8\nlimit = np.sqrt(6/(fan_in + fan_out))\nprint('limit:', limit)\nweights = np.random.uniform(-1*limit, limit, size=(100, 100))\nprint(weights)\nprint('weights mean:',weights.mean(), 'std:', weights.std(), 'sum:', weights.sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:30:18.1517Z","iopub.execute_input":"2021-07-21T12:30:18.152064Z","iopub.status.idle":"2021-07-21T12:30:18.163591Z","shell.execute_reply.started":"2021-07-21T12:30:18.152029Z","shell.execute_reply":"2021-07-21T12:30:18.162275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### He initialization - 정규분포(he_normal), 균일분포(he_uniform) ","metadata":{}},{"cell_type":"code","source":"fan_in = 10\nfan_out = 8\nscale_value = np.sqrt(2/(fan_in))\nprint('scale:', scale_value)\nweights = np.random.normal(loc=0.0, scale=scale_value, size=(100, 100))\nprint(weights)\nprint('weights mean:',weights.mean(), 'std:', weights.std(), 'sum:', weights.sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:32:28.304974Z","iopub.execute_input":"2021-07-21T12:32:28.305347Z","iopub.status.idle":"2021-07-21T12:32:28.315909Z","shell.execute_reply.started":"2021-07-21T12:32:28.305316Z","shell.execute_reply":"2021-07-21T12:32:28.314588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fan_in = 10\nfan_out = 8\nlimit = np.sqrt(6/(fan_in))\nprint('limit:', limit)\nweights = np.random.uniform(-1*limit, limit, size=(100, 100))\nprint(weights)\nprint('weights mean:',weights.mean(), 'std:', weights.std(), 'sum:', weights.sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:32:28.787299Z","iopub.execute_input":"2021-07-21T12:32:28.787622Z","iopub.status.idle":"2021-07-21T12:32:28.798601Z","shell.execute_reply.started":"2021-07-21T12:32:28.787595Z","shell.execute_reply":"2021-07-21T12:32:28.797586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### weight 초기화를 He Normal로 변경 후 성능 검증\n* Keras Conv2D의 기본 weight 초기화는 glorot_uniform임. 이를 he_normal로 변경 후 동일 모델로 성능 테스트 \n* label은 원-핫 인코딩을 적용 ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    labels = labels.squeeze()\n    \n    return images, labels\n\n# 0 ~ 1사이값 float32로 변경하는 함수 호출 한 뒤 OHE 적용 \ndef get_preprocessed_ohe(images, labels):\n    images, labels = get_preprocessed_data(images, labels)\n    # OHE 적용 \n    oh_labels = to_categorical(labels)\n    return images, oh_labels\n\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\ntrain_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\ntest_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\nprint(train_images.shape, train_oh_labels.shape, test_images.shape, test_oh_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:33:30.925721Z","iopub.execute_input":"2021-07-21T12:33:30.926046Z","iopub.status.idle":"2021-07-21T12:33:32.441826Z","shell.execute_reply.started":"2021-07-21T12:33:30.926016Z","shell.execute_reply":"2021-07-21T12:33:32.44068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\n# 이번엔 He로 해보자. 정말 성능이 좋아질까?\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_normal')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\nx = Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\nx = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:33:44.824274Z","iopub.execute_input":"2021-07-21T12:33:44.824725Z","iopub.status.idle":"2021-07-21T12:33:44.983561Z","shell.execute_reply.started":"2021-07-21T12:33:44.82466Z","shell.execute_reply":"2021-07-21T12:33:44.98267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer는 Adam으로 설정하고, label값이 원-핫 인코딩이므로 loss는 categorical_crossentropy 임. \nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=train_images, y=train_oh_labels, batch_size=64, epochs=30, validation_split=0.15 )","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:33:45.774946Z","iopub.execute_input":"2021-07-21T12:33:45.775312Z","iopub.status.idle":"2021-07-21T12:36:10.059271Z","shell.execute_reply.started":"2021-07-21T12:33:45.77528Z","shell.execute_reply":"2021-07-21T12:36:10.058353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_history(history)\n\n# 테스트 데이터로 성능 평가 -> 별로 차이 없음 \nmodel.evaluate(test_images, test_oh_labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:59:12.883891Z","iopub.execute_input":"2021-07-21T12:59:12.884352Z","iopub.status.idle":"2021-07-21T12:59:14.553331Z","shell.execute_reply.started":"2021-07-21T12:59:12.884303Z","shell.execute_reply":"2021-07-21T12:59:14.552366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Batch Normalization을 모델에 적용 후 성능 검증","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport random as python_random\n\nnp.random.seed(2021)\npython_random.seed(2021)\ntf.random.set_seed(2021)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:59:17.006864Z","iopub.execute_input":"2021-07-21T12:59:17.007239Z","iopub.status.idle":"2021-07-21T12:59:17.02401Z","shell.execute_reply.started":"2021-07-21T12:59:17.007207Z","shell.execute_reply":"2021-07-21T12:59:17.023049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(input_tensor)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same')(x)\nx = Activation('relu')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:59:33.130544Z","iopub.execute_input":"2021-07-21T12:59:33.130926Z","iopub.status.idle":"2021-07-21T12:59:33.596635Z","shell.execute_reply.started":"2021-07-21T12:59:33.130889Z","shell.execute_reply":"2021-07-21T12:59:33.595765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label값이 원-핫 인코딩이 아니므로 loss는 categorical_crossentropy 임. \nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=train_images, y=train_oh_labels, batch_size=64, epochs=30, validation_split=0.15)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T12:59:33.59924Z","iopub.execute_input":"2021-07-21T12:59:33.6004Z","iopub.status.idle":"2021-07-21T13:02:19.962592Z","shell.execute_reply.started":"2021-07-21T12:59:33.600358Z","shell.execute_reply":"2021-07-21T13:02:19.961674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:02:19.964702Z","iopub.execute_input":"2021-07-21T13:02:19.965083Z","iopub.status.idle":"2021-07-21T13:02:21.514523Z","shell.execute_reply.started":"2021-07-21T13:02:19.965043Z","shell.execute_reply":"2021-07-21T13:02:21.513572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### He Normal 적용 후 Batch Normalization","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ninput_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n#x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(input_tensor)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = Activation('relu')(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=2)(x)\n\n# cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\nx = Flatten(name='flatten')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(300, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.3)(x)\noutput = Dense(10, activation='softmax', name='output')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:02:21.517795Z","iopub.execute_input":"2021-07-21T13:02:21.518088Z","iopub.status.idle":"2021-07-21T13:02:21.700356Z","shell.execute_reply.started":"2021-07-21T13:02:21.51806Z","shell.execute_reply":"2021-07-21T13:02:21.699449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=train_images, y=train_oh_labels, batch_size=64, epochs=30, validation_split=0.15)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:02:21.702617Z","iopub.execute_input":"2021-07-21T13:02:21.703003Z","iopub.status.idle":"2021-07-21T13:05:08.355218Z","shell.execute_reply.started":"2021-07-21T13:02:21.702961Z","shell.execute_reply":"2021-07-21T13:05:08.354317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:05:08.356997Z","iopub.execute_input":"2021-07-21T13:05:08.357431Z","iopub.status.idle":"2021-07-21T13:05:09.178215Z","shell.execute_reply.started":"2021-07-21T13:05:08.357388Z","shell.execute_reply":"2021-07-21T13:05:09.177395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(np.expand_dims(test_images[0], axis=0))\npredicted_class = np.argmax(preds, axis=1)\nprint('예측 클래스 값:', predicted_class)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:05:09.181807Z","iopub.execute_input":"2021-07-21T13:05:09.182089Z","iopub.status.idle":"2021-07-21T13:05:09.368294Z","shell.execute_reply.started":"2021-07-21T13:05:09.182062Z","shell.execute_reply":"2021-07-21T13:05:09.36733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습 시 데이터를 섞는 shuffle 적용 유무에 따른  성능 테스트","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport random as python_random\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# seed 를 설정해서 학습시마다 동일한 결과 유도. 불행히도 의도한 대로 동작하지 않음. \ndef set_random_seed(seed_value):\n    np.random.seed(seed_value)\n    python_random.seed(seed_value)\n    tf.random.set_seed(seed_value)\n\n# 0 ~ 1사이값의 float32로 변경하는 함수\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\n# 0 ~ 1사이값 float32로 변경하는 함수 호출 한 뒤 OHE 적용 \ndef get_preprocessed_ohe(images, labels):\n    images, labels = get_preprocessed_data(images, labels)\n    # OHE 적용 \n    oh_labels = to_categorical(labels)\n    return images, oh_labels\n\n# 학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \ndef get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n    # 학습 및 테스트 데이터 세트를  0 ~ 1사이값 float32로 변경 및 OHE 적용. \n    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n    \n    # 학습 데이터를 검증 데이터 세트로 다시 분리\n    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n    \n    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels ) ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:05:09.370277Z","iopub.execute_input":"2021-07-21T13:05:09.370634Z","iopub.status.idle":"2021-07-21T13:05:09.987746Z","shell.execute_reply.started":"2021-07-21T13:05:09.370596Z","shell.execute_reply":"2021-07-21T13:05:09.986848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\n# random seed는 2021로 고정.\nset_random_seed(2021)\n# CIFAR10 데이터 재 로딩 및 Scaling/OHE 전처리 적용하여 학습/검증/데이터 세트 생성. \n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\nprint(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021)\n\nprint(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:05:09.989213Z","iopub.execute_input":"2021-07-21T13:05:09.989589Z","iopub.status.idle":"2021-07-21T13:05:11.77319Z","shell.execute_reply.started":"2021-07-21T13:05:09.98955Z","shell.execute_reply":"2021-07-21T13:05:11.772096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model 생성을 위한 별도 함수 생성","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\ndef create_model():\n    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n\n    #x = Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu')(input_tensor)\n    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(filters=64, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=64, kernel_size=3, padding='same')(x)\n    x = Activation('relu')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=2)(x)\n\n    x = Conv2D(filters=128, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters=128, kernel_size=3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=2)(x)\n\n    # cifar10의 클래스가 10개 이므로 마지막 classification의 Dense layer units갯수는 10\n    x = Flatten(name='flatten')(x)\n    x = Dropout(rate=0.5)(x)\n    x = Dense(300, activation='relu', name='fc1')(x)\n    x = Dropout(rate=0.3)(x)\n    output = Dense(10, activation='softmax', name='output')(x)\n\n    model = Model(inputs=input_tensor, outputs=output)\n    #model.summary()\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:05:11.774721Z","iopub.execute_input":"2021-07-21T13:05:11.77537Z","iopub.status.idle":"2021-07-21T13:05:11.791025Z","shell.execute_reply.started":"2021-07-21T13:05:11.775327Z","shell.execute_reply":"2021-07-21T13:05:11.789949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### shuffle을 False/True 변경하면서 테스트 ","metadata":{}},{"cell_type":"code","source":"model = create_model()\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n# 먼저 shuffle을 false로 테스트 \nnoshuffle_history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=64, epochs=30, shuffle=False, \n                    validation_data=(val_images, val_oh_labels))\nevaluation_result = model.evaluate(test_images, test_oh_labels, batch_size=64)\nprint('#### 테스트 세트로 evaluation 결과 :', evaluation_result)\n\n# model이 반복적으로 메모리 차지하는것을 없애기 위해서 수행. \ntf.keras.backend.clear_session()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-21T13:05:11.792506Z","iopub.execute_input":"2021-07-21T13:05:11.793124Z","iopub.status.idle":"2021-07-21T13:07:59.367434Z","shell.execute_reply.started":"2021-07-21T13:05:11.793077Z","shell.execute_reply":"2021-07-21T13:07:59.366396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### shuffle을 True로 변경하고 학습 및 테스트","metadata":{}},{"cell_type":"code","source":"model = create_model()\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n# shuffle을 True로 변경하여 학습 및 테스트\nshuffle_history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=64, epochs=30, shuffle=True, \n                    validation_data=(val_images, val_oh_labels))\nevaluation_result = model.evaluate(test_images, test_oh_labels, batch_size=64)\nprint('#### 테스트 세트로 evaluation 결과 :', evaluation_result)\n\ntf.keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:07:59.368965Z","iopub.execute_input":"2021-07-21T13:07:59.369392Z","iopub.status.idle":"2021-07-21T13:10:47.707516Z","shell.execute_reply.started":"2021-07-21T13:07:59.369351Z","shell.execute_reply":"2021-07-21T13:10:47.706658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 위에서 수행한 Shuffle테스트 시 validation 데이터 기반 성능 검증 시각화 ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history_shuffle(noshuffle_history, shuffle_history):\n    # subplot 2개 그릴 것이다 선언\n    figure, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n    # shuffle과 no shuffle의 validation accuracy 비교 \n    axs[0].plot(noshuffle_history.history['val_accuracy'], label='no shuffle acc')\n    axs[0].plot(shuffle_history.history['val_accuracy'], label='shuffle acc')\n    # shuffle과 no shuffle의 validation loss 비교 \n    axs[1].plot(noshuffle_history.history['val_loss'], label='no shuffle loss')\n    axs[1].plot(shuffle_history.history['val_loss'], label='shuffle loss')\n    axs[0].legend()\n    axs[1].legend()\n\nshow_history_shuffle(noshuffle_history, shuffle_history)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:10:47.708991Z","iopub.execute_input":"2021-07-21T13:10:47.709394Z","iopub.status.idle":"2021-07-21T13:10:48.084346Z","shell.execute_reply.started":"2021-07-21T13:10:47.709355Z","shell.execute_reply":"2021-07-21T13:10:48.083073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### batch 크기를 32, 64, 256, 512로 변경하면서 테스트","metadata":{}},{"cell_type":"code","source":"b_sizes = [32, 64, 256, 512]\nhistories = []\nevaluations = []\nfor b_size in b_sizes:\n    model = create_model()\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    # batch_size를 순차적으로 32, 64, 256, 512로 변경하여 학습 및 evaluation 수행. \n    print('##### batch size :', b_size, '학습 #####')\n    history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=b_size, epochs=30, \n                        shuffle=True, validation_data=(val_images, val_oh_labels))\n    # batch size별 학습 history 결과 저장. \n    histories.append(history)\n    # 테스트 세트로 evaluation 수행하고 batch size별 결과 저장. \n    evaluation_result = model.evaluate(test_images, test_oh_labels, batch_size=b_size)\n    print('#### 테스트 세트로 evaluation 결과 :', evaluation_result)\n    evaluations.append(evaluation_result)\n    \n    tf.keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:10:48.086246Z","iopub.execute_input":"2021-07-21T13:10:48.08683Z","iopub.status.idle":"2021-07-21T13:21:18.021455Z","shell.execute_reply.started":"2021-07-21T13:10:48.086787Z","shell.execute_reply":"2021-07-21T13:21:18.020568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history_batch(histories):\n    figure, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))  \n    # batch 크기별 validation accuracy 비교 \n    axs[0].plot(histories[0].history['val_accuracy'], label='batch 32 acc')\n    axs[0].plot(histories[1].history['val_accuracy'], label='batch 64 acc')\n    axs[0].plot(histories[2].history['val_accuracy'], label='batch 256 acc')\n    axs[0].plot(histories[3].history['val_accuracy'], label='batch 512 acc')\n    \n    # batch 크기별 validation loss 비교\n    axs[1].plot(histories[0].history['val_loss'], label='batch 32 loss')\n    axs[1].plot(histories[1].history['val_loss'], label='batch 64 loss')\n    axs[1].plot(histories[2].history['val_loss'], label='batch 256 loss')\n    axs[1].plot(histories[3].history['val_loss'], label='batch 512 loss')\n    \n    axs[0].legend()\n    axs[1].legend()\n\nshow_history_batch(histories)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T13:21:18.022847Z","iopub.execute_input":"2021-07-21T13:21:18.023273Z","iopub.status.idle":"2021-07-21T13:21:18.446064Z","shell.execute_reply.started":"2021-07-21T13:21:18.023207Z","shell.execute_reply":"2021-07-21T13:21:18.445033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}