{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-24T08:56:32.216703Z","iopub.execute_input":"2021-07-24T08:56:32.217028Z","iopub.status.idle":"2021-07-24T08:56:32.226842Z","shell.execute_reply.started":"2021-07-24T08:56:32.216953Z","shell.execute_reply":"2021-07-24T08:56:32.225968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conv2D 적용하기\n* Conv2D() 를 모델에 적용 시에는 반드시 입력은 배치 크기를 제외하고 3차원이 되어야 함(즉 배치를 포함하면 4차원)  ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D\nfrom tensorflow.keras.models import Model\n\n# RGB: (28, 28, 3) / greyscale: (28, 28, 1)\ninput_tensor = Input(shape=(28, 28, 1)) # cf. (28, 28) -> (28, 28, 1) 이때 1은 채널의 차원\n# Conv: 배치를 제외해. 무조건 3차원을 받음. 받고 나서 배치(4차원) \nx = Conv2D(filters=4, kernel_size=3, strides=1, padding='same', activation='relu')(input_tensor) # output: feature map(배치를 제외한 3차원)\nprint('x type:', type(x), 'x:', x) # 피처맵: 3차원!! 배치 사이즈 제외!!\n# 필터: 커널이 여러 개가 모임 -> 커널은 2차원 -> 1개의 필터는 무조건 3차원 (channels = kernels) \n# filters=4: shape=(None, 28, 28, 4) -> 필터 1개는 무조건 3차원... 피처맵도 3차원... -> 여기 이해가 가장 어렵","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:07:13.090927Z","iopub.execute_input":"2021-07-24T09:07:13.091261Z","iopub.status.idle":"2021-07-24T09:07:13.111075Z","shell.execute_reply.started":"2021-07-24T09:07:13.09123Z","shell.execute_reply":"2021-07-24T09:07:13.110049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_grey = Input(shape=(28, 28, 1)) # greyscale\n# input_rgb = Input(shape=(28, 28, 3)) # RGB","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x = Conv2D(filters=4, \n#            kernel_size=3, \n#            strides=1, \n#            padding='same', \n#            activation='relu') \n#            (input_tensor) # output: feature map(배치를 제외한 3차원 형태)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pooling 적용하기","metadata":{}},{"cell_type":"code","source":"input_tensor = Input(shape=(28, 28, 1)) \nx = Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu')(input_tensor)\nx = MaxPooling2D(2)(x)\nprint(x)\n# filters=16: shape=(None, 14, 14, 16)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:16:13.488774Z","iopub.execute_input":"2021-07-24T09:16:13.489148Z","iopub.status.idle":"2021-07-24T09:16:13.514937Z","shell.execute_reply.started":"2021-07-24T09:16:13.489119Z","shell.execute_reply":"2021-07-24T09:16:13.513905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN 모델 생성","metadata":{}},{"cell_type":"code","source":"input_tensor = Input(shape=(28, 28, 1)) # (None, 28, 28, 1)\nx = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu')(input_tensor) # (None, 28, 28, 32)\nx = Conv2D(filters=64, kernel_size=3, activation='relu')(x) # (None, 26, 26, 64)\nx = MaxPooling2D(2)(x) # # (None, 13, 13, 64)\n\nmodel = Model(inputs=input_tensor, outputs=x)\nmodel.summary()\n\n### 중요\n# 하나의 필터/피처맵은 무조건 3차원이다.\n# Conv에서 말하는 3차원은 배치를 제외한 3차원이다.\n# 여러 개의 커널 = 필터 -> 필터의 채널(커널) 개수는 keras가 자동적으로 맞춰줌(?) \n# Params: 3*3*32*64+64 = 18496 (=weight의 개수)\n# 3*3*32: 하나의 필터(3차원)\n# *64: 필터의 개수\n# +64: bias","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:16:14.561252Z","iopub.execute_input":"2021-07-24T09:16:14.561608Z","iopub.status.idle":"2021-07-24T09:16:14.602696Z","shell.execute_reply.started":"2021-07-24T09:16:14.561577Z","shell.execute_reply":"2021-07-24T09:16:14.601929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Flatten\n\ninput_tensor = Input(shape=(28, 28, 1))\nx = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu')(input_tensor)\nx = Conv2D(filters=64, kernel_size=3, activation='relu')(x)\nx = MaxPooling2D(2)(x) # pooling은 이미 나온 피처맵을 줄이는 거니까 학습할 파라미터는 없음 \n\n# 3차원으로 되어있는 Feature map 결과를 Fully Connected 연결하기 위해서는 Flatten()을 적용해야함. \n# Flatten(): 3차원 -> 1차원으로 변환하는 작업 \nx = Flatten()(x)\nx = Dense(100, activation='relu')(x) # 바로 붙이면 손실이 많을 수 있으니 Dense layer 추가 -> Dense를 하는 순간 파라미터 급 증가 -> 오버피팅 위험 -> dropout\noutput = Dense(10, activation='softmax')(x) # 최종 classification을 위한 softmax\nmodel = Model(inputs=input_tensor, outputs=output)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:16:15.69808Z","iopub.execute_input":"2021-07-24T09:16:15.698397Z","iopub.status.idle":"2021-07-24T09:16:15.749687Z","shell.execute_reply.started":"2021-07-24T09:16:15.698367Z","shell.execute_reply":"2021-07-24T09:16:15.748898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor = Input(shape=(28, 28, 1))\nx = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu')(input_tensor) \nx = Conv2D(filters=64, kernel_size=3, activation='relu')(x) \nx = MaxPooling2D(2)(x) \nx = Flatten()(x) \nx = Dense(100, activation='relu')(x) \nx = Dropout(0.4)(x) \noutput = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=input_tensor, outputs=output)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:29:52.841691Z","iopub.execute_input":"2021-07-24T09:29:52.842012Z","iopub.status.idle":"2021-07-24T09:29:52.892304Z","shell.execute_reply.started":"2021-07-24T09:29:52.841981Z","shell.execute_reply":"2021-07-24T09:29:52.891356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_tensor = Input(shape=(28, 28, 1)) # (None, 28, 28, 1)\n# x = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu')(input_tensor) # (None, 28, 28, 32)\n# x = Conv2D(filters=64, kernel_size=3, activation='relu')(x) # (None, 26, 26, 64)\n# x = MaxPooling2D(2)(x) # # (None, 13, 13, 64) cf. pooling은 이미 나온 피처맵을 줄이는 거니까 학습할 파라미터는 없음\n# x = Flatten()(x) # Flatten(): 3차원 피처맵 -> 1차원 FC로 연결\n# x = Dense(100, activation='relu')(x) # 바로 붙이면 손실이 많을 수 있으니 Dense layer 추가 \n# x = Dropout(0.4)(x) # Dense를 추가하는 순간 파라미터 급 증가 -> 오버피팅 위험 -> dropout\n# output = Dense(10, activation='softmax')(x) # 최종 classification을 위한 softmax 연결\n\n# model = Model(inputs=input_tensor, outputs=output)\n# model.summary()\n\n### 중요\n# 하나의 필터/피처맵은 무조건 3차원이다.\n# Conv에서 말하는 3차원은 배치를 제외한 3차원이다.\n# 여러 개의 커널 = 필터 -> 필터의 채널(커널) 개수는 keras가 자동적으로 맞춰준다. \n# Params: 3*3*32*64+64 = 18496 (=weight의 개수)\n# 3*3*32: 하나의 필터(3차원)\n# *64: 필터의 개수\n# +64: bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fashion MNIST 데이터 전처리후 모델 학습","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import fashion_mnist\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# 전체 6만개 데이터 중, 5만개는 학습 데이터용, 1만개는 테스트 데이터용으로 분리\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\n# 0 ~ 1사이값 float32로 변경하는 함수 호출 한 뒤 OHE 적용 \ndef get_preprocessed_ohe(images, labels):\n    images, labels = get_preprocessed_data(images, labels)\n    # OHE 적용 \n    oh_labels = to_categorical(labels)\n    return images, oh_labels\n\n# 학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \ndef get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n    # 학습 및 테스트 데이터 세트를  0 ~ 1사이값 float32로 변경 및 OHE 적용. \n    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n    \n    # 학습 데이터를 검증 데이터 세트로 다시 분리\n    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n    \n    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels ) \n\n\n# Fashion MNIST 데이터 재 로딩 및 전처리 적용하여 학습/검증/데이터 세트 생성. \n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\nprint(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021)\nprint(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:19:27.99496Z","iopub.execute_input":"2021-07-21T11:19:27.995275Z","iopub.status.idle":"2021-07-21T11:19:30.341402Z","shell.execute_reply.started":"2021-07-21T11:19:27.995246Z","shell.execute_reply":"2021-07-21T11:19:30.339663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import Accuracy\n\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:19:44.780659Z","iopub.execute_input":"2021-07-21T11:19:44.781002Z","iopub.status.idle":"2021-07-21T11:19:44.796701Z","shell.execute_reply.started":"2021-07-21T11:19:44.780972Z","shell.execute_reply":"2021-07-21T11:19:44.795816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=30, validation_data=(val_images, val_oh_labels))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:19:47.246837Z","iopub.execute_input":"2021-07-21T11:19:47.247188Z","iopub.status.idle":"2021-07-21T11:21:09.430869Z","shell.execute_reply.started":"2021-07-21T11:19:47.247158Z","shell.execute_reply":"2021-07-21T11:21:09.429846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 모델 성능 평가","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history(history):\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend()\n    \nshow_history(history)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:21:21.827916Z","iopub.execute_input":"2021-07-21T11:21:21.828273Z","iopub.status.idle":"2021-07-21T11:21:22.058884Z","shell.execute_reply.started":"2021-07-21T11:21:21.828242Z","shell.execute_reply":"2021-07-21T11:21:22.058095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 테스트 데이터 세트로 모델 성능 검증\nmodel.evaluate(test_images, test_oh_labels, batch_size=256, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:21:24.200003Z","iopub.execute_input":"2021-07-21T11:21:24.200361Z","iopub.status.idle":"2021-07-21T11:21:24.495192Z","shell.execute_reply.started":"2021-07-21T11:21:24.20033Z","shell.execute_reply":"2021-07-21T11:21:24.494419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropout을 적용하여 Fully Connected Layer의 오버피팅 조정\n* CNN은 일반적으로 Dense Layer보다는 파라미터수(weight 수) 작음\n* 하지만 많은 Filter 들을 적용하고 이를  Fully Connected Layer로 연결 시 파라미터 수가 늘어남. \n* Flatten() 이후 Dropout을 적용하여 특정 비율로 FC Layer 연결을 누락 적용. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Flatten, Dropout\n\ninput_tensor = Input(shape=(28, 28, 1))\nx = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu')(input_tensor)\nx = Conv2D(filters=64, kernel_size=3, activation='relu')(x)\nx = MaxPooling2D(2)(x)\n\nx = Flatten()(x) # Flatten -> Dense를 FC로 연결하면 overfitting의 위험이 있음 \nx = Dropout(rate=0.5)(x) # 일반적인 것에도 확장되는 모델을 만들기 위해 Dropout layer 추가\nx = Dense(100, activation='relu')(x)\noutput = Dense(10, activation='softmax')(x)\nmodel = Model(inputs=input_tensor, outputs=output)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:28:39.379981Z","iopub.execute_input":"2021-07-24T09:28:39.380301Z","iopub.status.idle":"2021-07-24T09:28:39.433354Z","shell.execute_reply.started":"2021-07-24T09:28:39.380265Z","shell.execute_reply":"2021-07-24T09:28:39.432589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=30, validation_data=(val_images, val_oh_labels))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:23:01.447844Z","iopub.execute_input":"2021-07-21T11:23:01.448259Z","iopub.status.idle":"2021-07-21T11:24:19.556495Z","shell.execute_reply.started":"2021-07-21T11:23:01.448228Z","shell.execute_reply":"2021-07-21T11:24:19.55569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_history(history)\nmodel.evaluate(test_images, test_oh_labels, batch_size=256, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:24:23.817575Z","iopub.execute_input":"2021-07-21T11:24:23.817976Z","iopub.status.idle":"2021-07-21T11:24:24.197317Z","shell.execute_reply.started":"2021-07-21T11:24:23.817946Z","shell.execute_reply":"2021-07-21T11:24:24.196553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D \n\ndef create_model():\n    input_tensor = Input(shape=(28, 28, 1))\n    x = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu')(input_tensor)\n    x = Conv2D(filters=64, kernel_size=3, activation='relu')(x)\n    x = MaxPooling2D(2)(x)\n\n    #x = Dropout(rate=0.5)(x) -> 조금 더 적극적인 dropout 모델\n    x = Flatten()(x) # 요즘은 flatten 안하는 추세 -> GlobalAveragePooling2D로 대체되는 중?\n    x = Dropout(rate=0.5)(x)\n    x = Dense(200, activation='relu')(x)\n    X = Dropout(rate=0.2)(x)\n    output = Dense(10, activation='softmax')(x)\n    model = Model(inputs=input_tensor, outputs=output)\n    model.summary()\n    \n    return model\n\nmodel = create_model()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:24:24.524885Z","iopub.execute_input":"2021-07-21T11:24:24.525205Z","iopub.status.idle":"2021-07-21T11:24:24.579929Z","shell.execute_reply.started":"2021-07-21T11:24:24.525177Z","shell.execute_reply":"2021-07-21T11:24:24.579153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=30, validation_data=(val_images, val_oh_labels))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:25:21.731849Z","iopub.execute_input":"2021-07-21T11:25:21.732183Z","iopub.status.idle":"2021-07-21T11:26:44.217823Z","shell.execute_reply.started":"2021-07-21T11:25:21.732153Z","shell.execute_reply":"2021-07-21T11:26:44.216987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_history(history)\nmodel.evaluate(test_images, test_oh_labels, batch_size=256, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:26:56.627681Z","iopub.execute_input":"2021-07-21T11:26:56.628013Z","iopub.status.idle":"2021-07-21T11:26:57.026043Z","shell.execute_reply.started":"2021-07-21T11:26:56.627982Z","shell.execute_reply":"2021-07-21T11:26:57.025242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 입력 이미지는 배치를 포함하여 4차원이 되어야 함(즉 배치를 제외하면 3차원)\n* Conv2D()는 입력으로 배치를 제외하고 3차원 입력이 되어야 함. \n* 하지만 2차원으로 입력해도 Input(shape=(28, 28, 1)) 에서 3차원으로 변경함. \n* 명확하게는 2차원 Grayscale이미지더라도 입력 numpy 이미지 배열에서 배치를 제외한 3차원 입력을 만들어 주는게 좋음. ","metadata":{}},{"cell_type":"code","source":"(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\nprint('before reshape:', train_images.shape, test_images.shape)\ntrain_images = np.reshape(train_images, (60000, 28, 28, 1))\ntest_images = np.reshape(test_images, (10000, 28, 28, 1))\nprint('after reshape:', train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n\n(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021)\nprint(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:26:59.950484Z","iopub.execute_input":"2021-07-21T11:26:59.950841Z","iopub.status.idle":"2021-07-21T11:27:00.643614Z","shell.execute_reply.started":"2021-07-21T11:26:59.95081Z","shell.execute_reply":"2021-07-21T11:27:00.642613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\nmodel = create_model()\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=30, validation_data=(val_images, val_oh_labels))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:27:04.84941Z","iopub.execute_input":"2021-07-21T11:27:04.849761Z","iopub.status.idle":"2021-07-21T11:28:26.879674Z","shell.execute_reply.started":"2021-07-21T11:27:04.84973Z","shell.execute_reply":"2021-07-21T11:28:26.878745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_history(history)\nmodel.evaluate(test_images, test_oh_labels, batch_size=256, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:28:26.881634Z","iopub.execute_input":"2021-07-21T11:28:26.88204Z","iopub.status.idle":"2021-07-21T11:28:27.454641Z","shell.execute_reply.started":"2021-07-21T11:28:26.881997Z","shell.execute_reply":"2021-07-21T11:28:27.453663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stride가 1이고 Padding이 없는 경우\n* I는 입력 Feature Map의 크기, F는 Filter의 크기(Kernel size), P는 Padding(정수), S는 Strides(정수)\n* O = (I - F + 2P)/2 + 1 = (5 - 3 + 0 )/1 + 1 = 3","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D\nfrom tensorflow.keras.models import Model\n\ninput_tensor = Input(shape=(5, 5, 1))\nx = Conv2D(filters=1, kernel_size=3, strides=1)(input_tensor)\nprint('x.shape:', x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:45:06.000317Z","iopub.execute_input":"2021-07-21T11:45:06.000672Z","iopub.status.idle":"2021-07-21T11:45:06.020389Z","shell.execute_reply.started":"2021-07-21T11:45:06.000639Z","shell.execute_reply":"2021-07-21T11:45:06.019638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stride가 1이고 Padding이 1인 경우\n* O = (I - F + 2P)/2 + 1 = (5 - 3 + 2 )/1 + 1 = 5","metadata":{}},{"cell_type":"code","source":"input_tensor = Input(shape=(5, 5, 1))\nx = Conv2D(filters=1, kernel_size=3, strides=1, padding='same')(input_tensor)\nprint('x.shape:', x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:45:38.327299Z","iopub.execute_input":"2021-07-21T11:45:38.327671Z","iopub.status.idle":"2021-07-21T11:45:38.3463Z","shell.execute_reply.started":"2021-07-21T11:45:38.327634Z","shell.execute_reply":"2021-07-21T11:45:38.345411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ZeroPadding2D Layer를 이용하여 padding을 수동으로 적용. \nfrom tensorflow.keras.layers import ZeroPadding2D\n\ninput_tensor = Input(shape=(5, 5, 1))\npadded_input = ZeroPadding2D(padding=1)(input_tensor)\nprint('shape after padding:', padded_input.shape)\nx = Conv2D(filters=1, kernel_size=3, strides=1)(padded_input)\nprint('x.shape:', x.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:45:56.543781Z","iopub.execute_input":"2021-07-21T11:45:56.544106Z","iopub.status.idle":"2021-07-21T11:45:56.56812Z","shell.execute_reply.started":"2021-07-21T11:45:56.544078Z","shell.execute_reply":"2021-07-21T11:45:56.567322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stride가 2이고 Padding이 없는 경우 \n* O = (I - F + 2P)/2 + 1 = (5 - 3)/2 + 1 = 2","metadata":{}},{"cell_type":"code","source":"input_tensor = Input(shape=(5, 5, 1))\nx = Conv2D(filters=1, kernel_size=3, strides=2)(input_tensor)\nprint('x.shape:', x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:46:00.489572Z","iopub.execute_input":"2021-07-21T11:46:00.489956Z","iopub.status.idle":"2021-07-21T11:46:00.507485Z","shell.execute_reply.started":"2021-07-21T11:46:00.489926Z","shell.execute_reply":"2021-07-21T11:46:00.506679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stride가 2이고 Padding은 1 적용\n* O = (I - F + 2P)/2 + 1 = (5 - 3 + 2)/2 + 1 = 3","metadata":{}},{"cell_type":"code","source":"input_tensor = Input(shape=(5, 5, 1))\npadded_input = ZeroPadding2D(padding=1)(input_tensor)\nprint('shape after padding:', padded_input.shape)\nx = Conv2D(filters=1, kernel_size=3, strides=2)(padded_input)\nprint('x.shape:', x.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 입력이 6X6에서 Stride가 2 적용\n* O = (I - F + 2P)/2 + 1 = (6 - 3 + 0)/2 + 1 = 2.5 = 2","metadata":{}},{"cell_type":"code","source":"input_tensor = Input(shape=(6, 6, 1))\nx = Conv2D(filters=1, kernel_size=3, strides=2)(input_tensor)\nprint('x.shape:', x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:50:07.132403Z","iopub.execute_input":"2021-07-21T11:50:07.13275Z","iopub.status.idle":"2021-07-21T11:50:07.148856Z","shell.execute_reply.started":"2021-07-21T11:50:07.13272Z","shell.execute_reply":"2021-07-21T11:50:07.147813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor = Input(shape=(6, 6, 1))\nx = Conv2D(filters=1, kernel_size=3, strides=2, padding='same')(input_tensor)\nprint('x.shape:', x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:50:07.301685Z","iopub.execute_input":"2021-07-21T11:50:07.302005Z","iopub.status.idle":"2021-07-21T11:50:07.318561Z","shell.execute_reply.started":"2021-07-21T11:50:07.301976Z","shell.execute_reply":"2021-07-21T11:50:07.317335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor = Input(shape=(6, 6, 1))\npadded_input = ZeroPadding2D(padding=1)(input_tensor)\nx = Conv2D(filters=1, kernel_size=3, strides=2, padding='valid')(padded_input)\nprint('x.shape:', x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:50:07.443263Z","iopub.execute_input":"2021-07-21T11:50:07.443567Z","iopub.status.idle":"2021-07-21T11:50:07.463313Z","shell.execute_reply.started":"2021-07-21T11:50:07.44354Z","shell.execute_reply":"2021-07-21T11:50:07.462331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor = Input(shape=(6, 6, 1))\npadded_input = ZeroPadding2D(padding=((1, 0),(1,0)))(input_tensor) # padding = 튜플: 위&왼쪽만 패딩\nx = Conv2D(filters=1, kernel_size=3, strides=2)(padded_input)\nprint('x.shape:', x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:50:07.617297Z","iopub.execute_input":"2021-07-21T11:50:07.617704Z","iopub.status.idle":"2021-07-21T11:50:07.637026Z","shell.execute_reply.started":"2021-07-21T11:50:07.617668Z","shell.execute_reply":"2021-07-21T11:50:07.636116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Maxpooling 적용","metadata":{}},{"cell_type":"code","source":"input_tensor = Input(shape=(223, 223, 1))\nx = MaxPooling2D(2)(input_tensor)\nprint('x.shape:', x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:50:07.961013Z","iopub.execute_input":"2021-07-21T11:50:07.961431Z","iopub.status.idle":"2021-07-21T11:50:07.97335Z","shell.execute_reply.started":"2021-07-21T11:50:07.961384Z","shell.execute_reply":"2021-07-21T11:50:07.972525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}